# AI-Based Recipe Generator & Recommender (LLM CookBook)

[![Python](https://img.shields.io/badge/python-%3E%3D3.8-blue)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-%3E%3D0.65-green)](https://fastapi.tiangolo.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-%3E%3D1.0-orange)](https://streamlit.io/)

> **Turn your pantry into a personalized recipe book**  
> â€” Enter your ingredients, filter recipes by preferences or allergens, and generate new recipes using advanced NLP techniques and LLMs.

---

## ðŸ“– Table of Contents

- [Features](#-features)
- [Tech Stack & Models](#-tech-stack--models)
- [Folder Structure](#-folder-structure)
- [Getting Started](#-getting-started)
  - [Prerequisites](#prerequisites)
  - [Clone & Install](#clone--install)
  - [Backend Setup](#backend-setup)
  - [Frontend Setup](#frontend-setup)
- [API Endpoints & Usage](#-api-endpoints--usage)
- [Next Steps & Improvements](#-next-steps--improvements)

---

## ðŸš€ Features

- **Ingredient Parsing:** Normalizes and cleans ingredient lists.
- **Recipe Search:** Semantic search using embeddings (MiniLM + FAISS).
- **Fallback LLM Generation:** Generates recipes with a local LLM (Ollama - Nous Hermes 2) when no matches are found.
- **Dietary & Allergen Filters:** Personalized filtering including dietary restrictions and allergens.
- **Ingredient Substitution:** Intelligent allergen substitution.
- **User-Friendly Frontend:** Interactive frontend built with Streamlit.

**Note:** This project is currently an **MVP** (Minimum Viable Product).

---

## ðŸ§° Tech Stack & Models

- **Backend:** FastAPI, Uvicorn
- **Frontend:** Streamlit
- **Embeddings & Search:** SentenceTransformers (`all-MiniLM-L6-v2`), FAISS
- **LLM:** Ollama (Nous-Hermes-2), optionally OpenAI GPT-4
- **Data Processing:** Pandas, NumPy

---

## YouTube Demo:

[https://youtu.be/dU8jeaVeWb0](https://youtu.be/dU8jeaVeWb0)

---

## ðŸ—‚ Folder Structure

```
llm\_cookbook/
â”œâ”€ backend/
â”‚   â”œâ”€ app/
â”‚   â”‚   â”œâ”€ main.py
â”‚   â”‚   â”œâ”€ config.py
â”‚   â”‚   â”œâ”€ routes/
â”‚   â”‚   â”‚   â”œâ”€ parse\_ingredients.py
â”‚   â”‚   â”‚   â”œâ”€ recipe\_search.py
â”‚   â”‚   â”‚   â”œâ”€ recipe\_generate.py
â”‚   â”‚   â”‚   â””â”€ substitute.py
â”‚   â”‚   â”œâ”€ services/
â”‚   â”‚   â”‚   â”œâ”€ vector\_index.py
â”‚   â”‚   â”‚   â””â”€ recipe\_generator.py
â”‚   â”‚   â””â”€ utils/
â”‚   â”‚       â”œâ”€ allergen\_utils.py
â”‚   â”‚       â”œâ”€ cuisine\_utils.py
â”‚   â”‚       â””â”€ substitute.py
â”‚   â”œâ”€ requirements.txt
â”‚   â””â”€ .venv/
â”œâ”€ frontend/
â”‚   â””â”€ streamlit\_app/
â”‚       â”œâ”€ app.py
â”‚       â””â”€ config.toml
â”œâ”€ data/
â”‚   â””â”€ recipes.csv
â”œâ”€ .gitignore
â””â”€ README.md
```

---

## âš™ Getting Started

### Prerequisites

- Python â‰¥ 3.8
- Git
- Ollama (Local LLM model inference)

Install Ollama and pull the required model:

```bash
ollama pull nous-hermes
```

### Clone & Install

```bash
git clone https://github.com/RajDesai-18/llm-cookbook.git
cd llm-cookbook

# Create and activate virtual environment
python -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r backend/requirements.txt
pip install streamlit
```

### Backend Setup

Start Ollama model:

```bash
ollama serve
```

Start FastAPI backend:

```bash
cd backend
uvicorn app.main:app --reload --port 8000
```

Visit FastAPI Swagger UI:
[http://localhost:8000/docs](http://localhost:8000/docs)

### Frontend Setup

Start Streamlit frontend:

```bash
cd frontend/streamlit_app
streamlit run app.py
```

Visit frontend app:
[http://localhost:8501](http://localhost:8501)

---

## ðŸ“¡ API Endpoints & Usage

### ðŸ”¹ Parse Ingredients

**Request:**

```http
POST /parse-ingredients
["2 cups tomatoes", "1 tsp salt"]
```

**Response:**

```json
{ "parsed": ["tomatoes", "salt"] }
```

### ðŸ”¹ Recipe Search

**Request:**

```http
POST /recipes/search
["tomatoes", "salt"]
```

**Response:**

```json
{
  "results": [
    {"title": "...", "ingredients": [...], "instructions": [...]},
    ...
  ]
}
```

### ðŸ”¹ Generate Recipe

**Request:**

```http
POST /recipes/generate
{
  "ingredients": ["tomatoes", "salt"],
  "dietary_preference": "vegan",
  "excluded_allergens": ["peanuts"]
}
```

**Response:**

```json
{
  "recipe": {
    "title": "...",
    "prep_time": "...",
    "cook_time": "...",
    "servings": "...",
    "ingredients": [...],
    "instructions": [...]
  }
}
```

---

## ðŸŽ¯ Next Steps & Improvements

- Ingredient Substitute Feature: Expand the existing ingredient substitution logic to suggest multiple alternatives and improve allergen substitution handling.
- Full-Fledged Frontend: Implement a comprehensive and responsive frontend using **React.js**, **TypeScript**, and **Tailwind CSS** for a richer user experience.
- Add unit and integration tests.
- Conduct user studies for usability feedback.
- Improve error handling and logging mechanisms.
- CI/CD integration and Docker containerization.
- Fine-tune the LLM on custom recipe data.

---
